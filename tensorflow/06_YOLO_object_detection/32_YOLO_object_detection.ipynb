{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf4XeX3xIi1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d642fb5f-6078-4b8e-b2b0-f4c8606b25c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations # For TPU\n",
        "\n",
        "import cv2 as cv\n",
        "import albumentations as A\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import io\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPool2D,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Input,\n",
        "    BatchNormalization,\n",
        "    Layer,\n",
        "    InputLayer,\n",
        "    Dropout,\n",
        "    Resizing,\n",
        "    Rescaling,\n",
        "    RandomFlip,\n",
        "    RandomRotation,\n",
        "    GlobalAveragePooling2D,\n",
        "    Add,\n",
        "    MultiHeadAttention,\n",
        "    Embedding,\n",
        "    LayerNormalization,\n",
        "    LeakyReLU,\n",
        ")\n",
        "from tensorflow.keras.losses import (\n",
        "    BinaryCrossentropy,\n",
        "    CategoricalCrossentropy,\n",
        "    SparseCategoricalCrossentropy,\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.metrics import (\n",
        "    CategoricalAccuracy,\n",
        "    TopKCategoricalAccuracy,\n",
        ")\n",
        "from tensorflow.keras.callbacks import (\n",
        "    Callback,\n",
        "    CSVLogger,\n",
        "    EarlyStopping,\n",
        "    LearningRateScheduler,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau,\n",
        ")\n",
        "from tensorflow.keras.regularizers import L2, L1\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "ROOT_DIR = \"/content/drive/MyDrive/tfds_data/pascal_voc_2012/\"\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjftGIKcPJYq",
        "outputId": "c4c932ea-9561-4dee-d8bd-cbc6c8d1a8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q kaggle\n",
        "# !mkdir ~/.kaggle/\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d huanghanchina/pascal-voc-2012"
      ],
      "metadata": {
        "id": "1A6EOD8NI5IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip \"/content/pascal-voc-2012.zip\" -d \"/content/drive/MyDrive/tfds_data/pascal_voc_2012/\"\n",
        "# !unzip \"/content/pascal-voc-2012.zip\" -d {ROOT_DIR}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XZzGXwfrQvGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data"
      ],
      "metadata": {
        "id": "5LEaCsf0XVcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMAGES = ROOT_DIR + \"VOC2012/JPEGImages/\"\n",
        "TRAIN_MAPS = ROOT_DIR + \"VOC2012/Annotations/\"\n",
        "VAL_IMAGES = ROOT_DIR + \"VOC2012/ValJPEGImages/\"\n",
        "VAL_MAPS = ROOT_DIR + \"VOC2012/ValAnnotations/\"\n",
        "\n",
        "os.makedirs(VAL_IMAGES, exist_ok=True)\n",
        "os.makedirs(VAL_MAPS, exist_ok=True)\n",
        "\n",
        "val_list=['2007_000027.jpg','2007_000032.jpg','2007_000033.jpg','2007_000039.jpg','2007_000042.jpg','2007_000061.jpg',\n",
        "          '2007_000063.jpg','2007_000068.jpg','2007_000121.jpg','2007_000123.jpg','2007_000129.jpg','2007_000170.jpg',\n",
        "          '2007_000175.jpg','2007_000187.jpg','2007_000241.jpg','2007_000243.jpg','2007_000250.jpg','2007_000256.jpg',\n",
        "          '2007_000272.jpg','2007_000323.jpg','2007_000332.jpg','2007_000333.jpg','2007_000346.jpg','2007_000363.jpg',\n",
        "          '2007_000364.jpg','2007_000392.jpg','2007_000423.jpg','2007_000452.jpg','2007_000464.jpg','2007_000480.jpg',\n",
        "          '2007_000491.jpg','2007_000504.jpg','2007_000515.jpg','2007_000528.jpg','2007_000529.jpg','2007_000549.jpg',\n",
        "          '2007_000559.jpg','2007_000572.jpg','2007_000584.jpg','2007_000629.jpg','2007_000636.jpg','2007_000645.jpg',\n",
        "          '2007_000648.jpg','2007_000661.jpg','2007_000663.jpg','2007_000664.jpg','2007_000676.jpg','2007_000713.jpg',\n",
        "          '2007_000720.jpg','2007_000727.jpg','2007_000733.jpg','2007_000738.jpg','2007_000762.jpg','2007_000768.jpg',\n",
        "          '2007_000783.jpg','2007_000793.jpg','2007_000799.jpg','2007_000804.jpg','2007_000807.jpg','2007_000822.jpg',\n",
        "          '2007_001299.jpg','2007_001311.jpg','2007_001321.jpg','2007_001340.jpg']"
      ],
      "metadata": {
        "id": "5Sam-1eGzk02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for name in val_list:\n",
        "#   shutil.move(TRAIN_IMAGES + name, VAL_IMAGES + name)\n",
        "# for name in val_list:\n",
        "#   # Removing jpg and appending xml as the extension.\n",
        "#   shutil.move(TRAIN_MAPS + name[:-3] + \"xml\", VAL_MAPS + name[:-3] + \"xml\")"
      ],
      "metadata": {
        "id": "3gH-WYRmzlpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "# Quantity of outputed bounding boxes\n",
        "B=2\n",
        "N_CLASSES = len(classes)\n",
        "H, W = 224, 224\n",
        "# Size of each cell\n",
        "SPLIT_SIZE = H//32\n",
        "N_EPOCHS = 135\n",
        "LR = 5e-4\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def preprocess_xml(filename):\n",
        "  tree = ET.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  sizeE = root.find(\"size\")\n",
        "  img_height = float(sizeE.find(\"height\").text)\n",
        "  img_width = float(sizeE.find(\"width\").text)\n",
        "  img_depth = float(sizeE.find(\"depth\").text)\n",
        "  class_dict = {classes[i]:i for i in range(len(classes))}\n",
        "  bounding_boxes = []\n",
        "\n",
        "  for objectE in root.findall(\"object\"):\n",
        "    for bbxE in objectE.iter(\"bndbox\"):\n",
        "      xmin = float(bbxE.find(\"xmin\").text)\n",
        "      ymin = float(bbxE.find(\"ymin\").text)\n",
        "      xmax = float(bbxE.find(\"xmax\").text)\n",
        "      ymax = float(bbxE.find(\"ymax\").text)\n",
        "      # print(xmin, ymin, xmax, ymax)\n",
        "      break # We break here because we want only one bbx for object.\n",
        "\n",
        "    class_name = objectE.find(\"name\").text\n",
        "    # x_center, y_center, width and height\n",
        "    # (all divided by img_width (or img_height in case of y) for normalization)\n",
        "    # NOTE: The center (x,y) is normalized with respect to the whole image,\n",
        "    # but we eventually will normalize it with respect to the cell width and\n",
        "    # height by multiplying x and y by the number of cells.\n",
        "    bbx = [\n",
        "        (xmin+xmax)/(2*img_width),\n",
        "        (ymin+ymax)/(2*img_height),\n",
        "        (xmax-xmin)/img_width,\n",
        "        (ymax-ymin)/img_height,\n",
        "        class_dict[class_name],\n",
        "    ]\n",
        "    bounding_boxes.append(bbx)\n",
        "\n",
        "  \"\"\"\n",
        "  bounding_boxes[b][0] >> x_center/img_width\n",
        "  bounding_boxes[b][1] >> y_center/img_height\n",
        "  bounding_boxes[b][2] >> width/img_width\n",
        "  bounding_boxes[b][3] >> height/img_height\n",
        "  bounding_boxes[b][4] >> class number\n",
        "  \"\"\"\n",
        "  # return bounding_boxes\n",
        "  return tf.convert_to_tensor(bounding_boxes) # For tf.numpy_function"
      ],
      "metadata": {
        "id": "oRBwWEb9RgxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_xml(TRAIN_MAPS + \"2007_000032.xml\") # This image was moved."
      ],
      "metadata": {
        "id": "VdStZYAXROd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31:41:00\n",
        "def generate_output_v1(bounding_boxes, length=None):\n",
        "  if length is None:\n",
        "    length = len(bounding_boxes)\n",
        "\n",
        "  # N_CLASSs+5 because we have the five first positions for:\n",
        "  # >> [0] = objectness/score (probability of having an object in the cell)\n",
        "  # >> [1] = x_center\n",
        "  # >> [2] = y_center\n",
        "  # >> [3] = cell width with respect to the img_width\n",
        "  # >> [4] = cell height with respect to the img_height\n",
        "  # >> [5:] = classes (1 for the correct class position and 0 for the others)\n",
        "  output_label = tf.Variable(lambda: tf.zeros([SPLIT_SIZE, SPLIT_SIZE, N_CLASSES+5], dtype=tf.float32))\n",
        "\n",
        "  # NOTE: We only need to iterate through the number of bbx of the image\n",
        "  # because all the other cells will have 0 for all positions of the tensor\n",
        "  # (due to the fact that they don't have objects on them).\n",
        "  for b in range(length):\n",
        "    # NOTE: We multiply it by the SPLIT_SIZE to make\n",
        "    # x and y to be with respect to the cell size.\n",
        "    # It was with respect to the img_width/img_height before.\n",
        "    grid_x = bounding_boxes[b][0]*SPLIT_SIZE\n",
        "    grid_y = bounding_boxes[b][1]*SPLIT_SIZE\n",
        "    # i for x of the cell in the image. (ex: 0)\n",
        "    # j for y of the cell in the image. (ex: 5)\n",
        "    i = int(grid_x)\n",
        "    j = int(grid_y)\n",
        "\n",
        "    # print(i, j, grid_x, grid_y)\n",
        "\n",
        "    \"\"\"\n",
        "    bounding_boxes[b][0] >> x_center/img_width\n",
        "    bounding_boxes[b][1] >> y_center/img_height\n",
        "    bounding_boxes[b][2] >> width/img_width\n",
        "    bounding_boxes[b][3] >> height/img_height\n",
        "    bounding_boxes[b][4] >> class number\n",
        "    \"\"\"\n",
        "    # if the score of the cell[i,j] is 0\n",
        "    if(output_label[i, j, 0] == 0):\n",
        "      # These 5 have already been explained in the first comment.\n",
        "      output_label[i, j, 0:5].assign([1., grid_x%1, grid_y%1, bounding_boxes[b][2], bounding_boxes[b][3]])\n",
        "      # Put 1. in the class position\n",
        "      output_label[i, j, 5+int(bounding_boxes[b][4])].assign(1.)\n",
        "\n",
        "  # This needs to be correctly indented and outside the loop, pal.\n",
        "  return output_label"
      ],
      "metadata": {
        "id": "eZq9AfyYdtzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbxs = preprocess_xml(VAL_MAPS+\"2007_000733.xml\")\n",
        "# tf.config.run_functions_eagerly(False)\n",
        "generate_output_v1(bbxs, len(bbxs)).shape\n",
        "bbxs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61nPNweluSR",
        "outputId": "6772095e-9ce0-4a23-b913-f23f01adf541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
              "array([[ 0.35666665,  0.46896553,  0.5       ,  0.8229885 , 14.        ],\n",
              "       [ 0.6122222 ,  0.73103446,  0.76666665,  0.537931  , 13.        ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 31:41:00\n",
        "# https://github.com/Neuralearn/deep-learning-with-tensorflow-2/blob/main/deep%20learning%20for%20computer%20vision/5-YOLO%20Object%20Detection%20from%20Scratch%20by%20Neuralearn.ai-.ipynb\n",
        "def generate_output_v2(bounding_boxes):\n",
        "  output_label = np.zeros([SPLIT_SIZE, SPLIT_SIZE, N_CLASSES+5], dtype=np.float32)\n",
        "\n",
        "  for b in range(len(bounding_boxes)):\n",
        "    # [..., ?, ?] because it will be batched (probably of size 32)\n",
        "    grid_x = bounding_boxes[..., b, 0]*SPLIT_SIZE\n",
        "    grid_y = bounding_boxes[..., b, 1]*SPLIT_SIZE\n",
        "    i = int(grid_x)\n",
        "    j = int(grid_y)\n",
        "\n",
        "    if(output_label[i, j, 0] == 0):\n",
        "      output_label[i, j, 0:5] = [1., grid_x%1, grid_y%1, bounding_boxes[..., b, 2], bounding_boxes[..., b, 3]]\n",
        "      output_label[i, j, 5+int(bounding_boxes[..., b, 4])] = 1.\n",
        "\n",
        "  # This needs to be correctly indented and outside the loop, pal.\n",
        "  return tf.convert_to_tensor(output_label)"
      ],
      "metadata": {
        "id": "lr3pjiB9wDEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: A.RandomCrop() and A.RandomScale() may result in a division by zero if\n",
        "# the bbx is too close to the borders.\n",
        "transforms = A.Compose([\n",
        "    A.Resize(H, W),\n",
        "    # A.RandomCrop(\n",
        "    #     width=np.random.randint(int(0.9*W),W),\n",
        "    #     height=np.random.randint(int(0.9*H), H),\n",
        "    #     p=0.5,\n",
        "    #     always_apply=False,\n",
        "    # ),\n",
        "    A.BBoxSafeRandomCrop(erosion_rate=0.2, p=1.0),\n",
        "    # A.RandomScale(scale_limit=0.1, interpolation=cv.INTER_LANCZOS4, p=0.5,),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Resize(H, W),\n",
        "], bbox_params=A.BboxParams(format=\"yolo\"))\n",
        "\n",
        "def aug_albument(img, bboxes):\n",
        "  # NOTE: remember that you must pass the bboxes in the format [[x_center, y_center, width, height, class], ...]\n",
        "  # KeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'\n",
        "  augmented = transforms(image=img, bboxes=bboxes)\n",
        "  return [tf.convert_to_tensor(augmented[\"image\"], dtype=tf.float32), tf.convert_to_tensor(augmented[\"bboxes\"], dtype=tf.float32)]"
      ],
      "metadata": {
        "id": "PsclVHJmFw0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_im_paths = []\n",
        "train_xml_paths = []\n",
        "val_im_paths = []\n",
        "val_xml_paths = []\n",
        "\n",
        "for p in os.listdir(TRAIN_IMAGES):\n",
        "  train_im_paths.append(TRAIN_IMAGES + p)\n",
        "  train_xml_paths.append(TRAIN_MAPS + p[:-3] + \"xml\")\n",
        "\n",
        "for p in os.listdir(VAL_IMAGES):\n",
        "  val_im_paths.append(VAL_IMAGES + p)\n",
        "  val_xml_paths.append(VAL_MAPS + p[:-3] + \"xml\")\n",
        "\n",
        "print(len(train_im_paths), len(train_xml_paths))\n",
        "print(len(val_im_paths), len(val_xml_paths))\n",
        "\n",
        "def get_imbboxes(im_path, xml_path):\n",
        "  img = tf.io.read_file(im_path)\n",
        "  img = tf.io.decode_jpeg(img)\n",
        "  img = tf.image.resize(img, size=(H, W))\n",
        "  img = tf.cast(img, dtype=tf.float32)\n",
        "\n",
        "  # This is necessary because preprocess_xml() is not made entirely of tensorflow\n",
        "  # operations only.\n",
        "  # NOTE: preprocess_xml MUST return a tensor.\n",
        "  bbxs = tf.numpy_function(func=preprocess_xml, inp=[xml_path], Tout=tf.float32)\n",
        "\n",
        "  # tf.ensure_shape(img, (H, W, 3))\n",
        "  # tf.ensure_shape(bbxs, [None, 5])\n",
        "\n",
        "  # print(\"-\"*30)\n",
        "  # print(img.shape)\n",
        "  # print(bbxs.shape)\n",
        "\n",
        "  return img, bbxs\n",
        "\n",
        "def augment_data(img, bboxes):\n",
        "  # We use Albumentations here because it involver transformations which involve changing the bboxes values.\n",
        "  img, bboxes = tf.numpy_function(func=aug_albument, inp=[img, bboxes], Tout=(tf.float32, tf.float32))\n",
        "  # These below don't require changes in the bboxes.\n",
        "  img = tf.image.random_brightness(img, max_delta=50.)\n",
        "  img = tf.image.random_hue(img, max_delta=0.5)\n",
        "  img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n",
        "  img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n",
        "\n",
        "  # tf.ensure_shape(img, (H, W, 3))\n",
        "  # tf.ensure_shape(bboxes, [None, 5])\n",
        "\n",
        "  return img, bboxes\n",
        "\n",
        "def preproces_bbxes(img, bboxes):\n",
        "  labels = tf.numpy_function(func=generate_output_v2, inp=[bboxes], Tout=tf.float32)\n",
        "  # tf.ensure_shape(img, (H, W, 3))\n",
        "  # tf.ensure_shape(labels, (SPLIT_SIZE, SPLIT_SIZE, 25))\n",
        "  return img, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM3kG2IU4mav",
        "outputId": "a4db5925-edbb-49b5-91fd-1e9b3b9d0e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17061 17061\n",
            "64 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_im_paths, train_xml_paths))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_im_paths, val_xml_paths))\n",
        "\n",
        "def ensure_shape(x, y):\n",
        "  # Ensures the dataset elements have a defined shape.\n",
        "  return (\n",
        "      tf.ensure_shape(x, (None, H, W, 3)),\n",
        "      tf.ensure_shape(y, (None, SPLIT_SIZE, SPLIT_SIZE, N_CLASSES+5))\n",
        "  )\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .map(get_imbboxes)\n",
        "    .map(augment_data)\n",
        "    .map(preproces_bbxes) # This goes AFTER augment_data\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(ensure_shape)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(get_imbboxes)\n",
        "    # .map(augment_data) # Why in the hell would we augment validation data?\n",
        "    .map(preproces_bbxes)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(ensure_shape)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "KGLCiuNyCawk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, j in train_dataset.take(1):\n",
        "#   # cv.imwrite(\"out1.png\", i.numpy())\n",
        "#   print(i.shape, j.shape)"
      ],
      "metadata": {
        "id": "CxXiDag8x2DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, j in train_dataset.take(1):\n",
        "#   # cv.imwrite(\"out2.png\", i.numpy())\n",
        "#   print(i.shape, j.shape)"
      ],
      "metadata": {
        "id": "NoTYyRHaFbp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(boxes1, boxes2):\n",
        "  # [..., 0] -> x_center (batched)\n",
        "  # [..., 1] -> y_center (batched)\n",
        "  # [..., 2] -> width    (batched)\n",
        "  # [..., 3] -> height   (batched)\n",
        "\n",
        "  # xmin, ymin, xmax, ymax\n",
        "  boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
        "                       boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
        "                       boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
        "                       boxes1[..., 1] + boxes1[..., 3] / 2.0,],\n",
        "                      axis=-1)\n",
        "\n",
        "  boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
        "                       boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
        "                       boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
        "                       boxes2[..., 1] + boxes2[..., 3] / 2.0,],\n",
        "                      axis=-1)\n",
        "\n",
        "  # intermin\n",
        "  lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
        "  # intermax\n",
        "  rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
        "  # print(lu, rd)\n",
        "\n",
        "  # Fomula for the area -> (xmax - xmin) * (ymax - ymin)\n",
        "  # (it's basically WIDTH * HEIGHT)\n",
        "  intersection = tf.maximum(0.0, rd - lu)\n",
        "  intersection_areas = intersection[..., 0] * intersection[..., 1]\n",
        "\n",
        "  # It's basically WIDTH * HEIGHT\n",
        "  areas_1 = boxes1[..., 2] * boxes1[..., 3]\n",
        "  areas_2 = boxes2[..., 2] * boxes2[..., 3]\n",
        "\n",
        "  union_areas = tf.maximum(areas_1 + areas_2 - intersection_areas, 1e-10)\n",
        "  return tf.clip_by_value(intersection_areas / union_areas, 0.0, 1.0)\n",
        "\n",
        "def difference(x, y):\n",
        "  return tf.reduce_sum(y-x)\n",
        "\n",
        "@tf.function\n",
        "def yolo_loss(y_true, y_pred, should_print=False):\n",
        "  \"\"\"\n",
        "  print(y_true.shape)         -> (?, 7, 7, 25)\n",
        "  print(y_true[0].shape)      -> (7, 7, 25)\n",
        "  print(y_true[..., 0].shape) -> (?, 7, 7)\n",
        "  \"\"\"\n",
        "  # [..., 0] because we want to know only the scores (chance of having an object).\n",
        "  target = y_true[..., 0]\n",
        "\n",
        "  # ======== Object Loss (for regions where we DO HAVE objects in the original image) ========\n",
        "  y_pred_extract = tf.gather_nd(y_pred, tf.where(target[:]==1))\n",
        "  y_target_extract = tf.gather_nd(y_true, tf.where(target[:]==1))\n",
        "\n",
        "  # ´rescaler´  contains [b, x, y] of the origin point (0,0) of the cell with respect to the whole image.\n",
        "  # EX: rescaler = [[0, 32, 128], [0, 96, 64]]\n",
        "  # >> SPLIT_SIZE is 7 for this case.\n",
        "  # >> 224 is the size of the image.\n",
        "  # >> 32 because: 224 / SPLIT_SIZE = 32\n",
        "  rescaler = tf.where(target[:]==1)*32\n",
        "\n",
        "  # rescaler_shape = rescaler.shape[0]\n",
        "  rescaler_shape = tf.shape(rescaler)[0]\n",
        "  # tf.print(rescaler_shape)\n",
        "\n",
        "  # Creating space for weight and height as well (horizontally).\n",
        "  # EX: upscaler_1 = [[32, 128, 0, 0], [96, 64, 0, 0]]\n",
        "  upscaler_1 = tf.concat([rescaler[:, 1:], tf.zeros([rescaler_shape, 2], dtype=tf.int64)], axis=-1)\n",
        "  # [32., 32., 224., 224.,]\n",
        "  # >> SPLIT_SIZE is 7 for this case.\n",
        "  # >> 224 is the size of the image.\n",
        "  # >> 32 because: 224 / SPLIT_SIZE = 32\n",
        "  target_upscaler_2 = tf.repeat([[32., 32., 224., 224.,]], repeats=[rescaler_shape], axis=0)\n",
        "  # y_target_extract[..., 1:5]\n",
        "  # >> ... because it may be batched\n",
        "  # >> 1:5 because we don't want the score, only (x, y, width, height).\n",
        "  # And we multiply each one (x, y, width and height) by 32, 32, 224 and 224, respectively\n",
        "  # in order to get the distance from (0, 0) of the cell to the (x, y) of the object (inside the cell).\n",
        "  target_upscaler_2 *= tf.cast(y_target_extract[..., 1:5], dtype=tf.float32)\n",
        "\n",
        "  pred_1_upscaler_2 = tf.repeat([[32.,32.,224.,224.,]], repeats=[rescaler_shape], axis=0)\n",
        "  # y_pred_extract[..., 1:5]\n",
        "  # >> ... because it may be batched\n",
        "  # >> 1:5 because we don't want the score, only (x, y, width, height).\n",
        "  pred_1_upscaler_2 *= tf.cast(y_pred_extract[..., 1:5], dtype=tf.float32)\n",
        "  # >> 6:10 because we don't want the SECOND score, BUT we want the second bbx x, y, width and height.\n",
        "  pred_2_upscaler_2 = tf.repeat([[32.,32.,224.,224.,]], repeats=[rescaler_shape], axis=0)\n",
        "  pred_2_upscaler_2 *= tf.cast(y_pred_extract[..., 6:10], dtype=tf.float32)\n",
        "\n",
        "  # If we add the x, y, width and height from the origin (0, 0) of the cell with\n",
        "  # the x, y, width and height of the origin with respect to the whole image,\n",
        "  # we get the x, y, width(0) and height(0) of the bbx with respect to the whole\n",
        "  # image.\n",
        "  # EX: [32., 32., 224., 224.,] + [18.89, 113.56, 56.778, 78.32,]\n",
        "  target_origin = tf.cast(upscaler_1, dtype=tf.float32) + target_upscaler_2\n",
        "  pred_1_origin = tf.cast(upscaler_1, dtype=tf.float32) + pred_1_upscaler_2\n",
        "  pred_2_origin = tf.cast(upscaler_1, dtype=tf.float32) + pred_2_upscaler_2\n",
        "\n",
        "  # This tells if the first or second bbx is closer in area to the target bbx (y_true):\n",
        "  # >> outputs 0 if it's the first.\n",
        "  # >> outputs 1 if it's the second.\n",
        "  # NOTE: How to interpret mask: EX: [0, 1]\n",
        "  # >> \"The first(0) bbx of the first pair of predictions has a higher IOU\"\n",
        "  # >> \"The second(1) bbx of the second pair of predictions has a higher IOU\"\n",
        "  mask = tf.math.greater(calculate_iou(target_origin, pred_1_origin), calculate_iou(target_origin, pred_2_origin))\n",
        "  mask = tf.cast(mask, dtype=tf.int32)\n",
        "\n",
        "  y_pred_joined = tf.transpose(tf.concat([tf.expand_dims(y_pred_extract[..., 0], axis=0), tf.expand_dims(y_pred_extract[..., 5], axis=0)], axis=0))\n",
        "\n",
        "  obj_pred = tf.gather_nd(y_pred_joined, tf.stack([tf.range(rescaler_shape), mask], axis=-1))\n",
        "\n",
        "  obj_loss = tf.math.abs(difference(tf.cast(obj_pred, dtype=tf.float32), tf.cast(tf.ones([rescaler_shape]), dtype=tf.float32)))\n",
        "\n",
        "  # ======== No Object Loss (for regions where we DO NOT HAVE objects in the original image) ========\n",
        "  y_pred_extract = tf.gather_nd(y_pred[..., 0:B*5], tf.where(target[:] == 0))\n",
        "  y_target_extract = tf.zeros([len(y_pred_extract)])\n",
        "\n",
        "  no_object_loss_1 = tf.math.abs(difference(tf.cast(y_pred_extract[..., 0], dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32)))\n",
        "  no_object_loss_2 = tf.math.abs(difference(tf.cast(y_pred_extract[..., 5], dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32)))\n",
        "\n",
        "  no_object_loss = no_object_loss_1 + no_object_loss_2\n",
        "\n",
        "  # ======== Object Class Loss ========\n",
        "  # tf.print(tf.shape(y_pred))\n",
        "  # tf.print(tf.shape(y_true))\n",
        "  y_pred_extract = tf.gather_nd(y_pred[..., 10:], tf.where(target[:]==1))\n",
        "  class_extract = tf.gather_nd(y_true[..., 5:], tf.where(target[:]==1))\n",
        "  # tf.print(tf.shape(y_pred_extract))\n",
        "  # tf.print(tf.shape(class_extract))\n",
        "\n",
        "  class_loss = tf.math.abs(difference(tf.cast(y_pred_extract, dtype=tf.float32), tf.cast(class_extract, dtype=tf.float32)))\n",
        "\n",
        "  # print(class_loss)\n",
        "\n",
        "  # ======== Object Bounding Box Loss ========\n",
        "  # For x_center and y_center\n",
        "  y_pred_extract = tf.gather_nd(y_pred[..., 0:B*5], tf.where(target[:]==1))\n",
        "  center_joined = tf.stack([y_pred_extract[..., 1:3], y_pred_extract[..., 6:8]], axis=1)\n",
        "\n",
        "  # Filtering for only x_center and y_center from the bbx with the higher IOU.\n",
        "  center_pred = tf.gather_nd(center_joined, tf.stack([tf.range(rescaler_shape), mask], axis=-1))\n",
        "  center_target = tf.gather_nd(y_true[..., 1:3], tf.where(target[:]==1))\n",
        "\n",
        "  center_loss = tf.math.abs(difference(tf.cast(center_pred, dtype=tf.float32), tf.cast(center_target, dtype=tf.float32)))\n",
        "\n",
        "  # For width and height\n",
        "  size_joined = tf.stack([y_pred_extract[..., 3:5], y_pred_extract[..., 8:10]], axis=1)\n",
        "  size_pred = tf.gather_nd(size_joined, tf.stack([tf.range(rescaler_shape), mask], axis=-1))\n",
        "  size_target = tf.gather_nd(y_true[..., 3:5], tf.where(target[:]==1))\n",
        "\n",
        "  # print(tf.math.abs(size_pred))\n",
        "  # print(tf.math.abs(size_target))\n",
        "  size_loss = tf.math.abs(difference(tf.cast(tf.math.sqrt(tf.math.abs(size_pred)), dtype=tf.float32), tf.cast(tf.math.sqrt(tf.math.abs(size_target)), dtype=tf.float32)))\n",
        "\n",
        "  box_loss = center_loss + size_loss\n",
        "\n",
        "  # ======== Final Loss ========\n",
        "  lambda_coord = 5.0\n",
        "  lambda_no_obj = 0.5\n",
        "\n",
        "  # Just to be sure\n",
        "  obj_loss = tf.math.abs(obj_loss)\n",
        "  no_object_loss = tf.math.abs(no_object_loss)\n",
        "  class_loss = tf.math.abs(class_loss)\n",
        "  box_loss = tf.math.abs(box_loss)\n",
        "\n",
        "  loss = tf.math.abs(obj_loss) + (lambda_no_obj * no_object_loss) + tf.cast(lambda_coord * box_loss, dtype=tf.float32) + tf.cast(class_loss, dtype=tf.float32)\n",
        "  loss = tf.reduce_mean(loss) # Just for tensorflow to understand the shape of `loss`.\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "hIKbO_woZzFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# x_center, y_center, width, height (all normalized)\n",
        "For [0.210784,0.616422,0.127451,0.232843,2], the cell indexes are calculated as follows:\n",
        ">> x = int(0.210784 * SPLIT_SIZE) = 1\n",
        ">> y = int(0.616422 * SPLIT_SIZE) = 4\n",
        "For [0.509804,0.411765,0.107843,0.245098,3], the cell indexes are calculated as:\n",
        ">> x = int(0.509804 * SPLIT_SIZE) = 3\n",
        ">> y = int(0.411765 * SPLIT_SIZE) = 2\n",
        "\"\"\"\n",
        "# y_true=generate_output_v2(np.array([[0.210784,0.616422,0.127451,0.232843,2]]))\n",
        "# y_true=generate_output_v2(np.array([[0.509804,0.411765,0.107843,0.245098,3]]))\n",
        "y_true=generate_output_v2(np.array([[0.509804,0.411765,0.107843,0.245098,3], [0.210784,0.616422,0.127451,0.232843,2]]))\n",
        "# y_true=generate_output_v2(np.array([]))\n",
        "y_true=np.expand_dims(y_true,axis=0)\n",
        "y_pred=np.random.normal(size = (1,7,7,N_CLASSES+5*B))\n",
        "\n",
        "# print(y_pred.shape)\n",
        "\n",
        "#                    5 info for bbx,            5 info for bbx,           20 classes\n",
        "y_pred[0][1][4] = [0.9,0.2,0.6,0.1,0.95,      1.0,0.47,0.31,0.12,0.23,   0.9,  0.8, 0.2, 0.6, 0.1, 0.5, 0.9,0.35, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,]\n",
        "# y_pred[0][1][4] = [0.9,0.47,0.31,0.12,0.23,     1.0,0.2,0.6,0.1,0.95,    0.9,  0.8, 0.2, 0.6, 0.1, 0.5, 0.9,0.35, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,]\n",
        "y_pred[0][3][2] = [0.3,0.01,0.08,0.11,0.54,   0.98,0.56,0.88,0.1,0.24,  0.09,0.018,0.22,0.16,0.01,0.05,0.99, 0.3, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,]\n",
        "# y_pred[0][3][2] = [0.3,0.01,0.08,0.11,0.54,   0.98,0.56,0.88,0.1,0.24,  0.09,0.018,0.22,0.16,0.01,0.05,0.99, 0.3, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,]\n",
        "\n",
        "yolo_loss(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I63nnLx8bFwL",
        "outputId": "09ac8a83-b59d-4b82-8c70-43d8e2b5d7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=19.728378>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FILTERS=512\n",
        "# 5 * B because it will be the number of bounding boxes outputed for a single\n",
        "# object. Ex: when B=2, we have 2 bounding boxes (each having 5 items due to the score being the first).\n",
        "OUTPUT_DIM=N_CLASSES + (5*B)\n",
        "\n",
        "# base_model = tf.keras.applications.resnet50.ResNet50(\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB1(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(H, W, 3),\n",
        ")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "\n",
        "    # `he_normal` is a weight initialization method designed for layers using rectified\n",
        "    # linear unit (ReLU) activation functions (or similar variants like LeakyReLU) which\n",
        "    # helps prevent vanishing and exploding gradients, ensuring that the weights remain\n",
        "    # at an appropriate scale for effective learning, especially in deep networks.\n",
        "    Conv2D(NUM_FILTERS, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\"),\n",
        "    BatchNormalization(),\n",
        "    # UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
        "    # NOTE: But if you're using an older version of tensorflow, it may be necessary.\n",
        "    # LeakyReLU(negative_slope=0.1),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "    Conv2D(NUM_FILTERS, kernel_size=(3,3) , padding=\"same\", kernel_initializer=\"he_normal\"),\n",
        "    BatchNormalization(),\n",
        "    # LeakyReLU(negative_slope=0.1),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "    Conv2D(NUM_FILTERS, kernel_size=(3,3) , padding=\"same\", kernel_initializer=\"he_normal\"),\n",
        "    BatchNormalization(),\n",
        "    # LeakyReLU(negative_slope=0.1),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "    Conv2D(NUM_FILTERS, kernel_size=(3,3) , padding=\"same\", kernel_initializer=\"he_normal\"),\n",
        "    # LeakyReLU(negative_slope=0.1),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "    # GlobalAveragePooling2D() # We don't want only the mean/representative value for each channel.\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(NUM_FILTERS, kernel_initializer=\"he_normal\"),\n",
        "    BatchNormalization(),\n",
        "    # LeakyReLU(negative_slope=0.1),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # 7 * 7 * 30\n",
        "    Dense(SPLIT_SIZE * SPLIT_SIZE * OUTPUT_DIM, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.Reshape((SPLIT_SIZE, SPLIT_SIZE, OUTPUT_DIM)),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "dhzNu4-XBZiH",
        "outputId": "a18b1534-4ce6-4df5-cd23-0351ad763da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ efficientnetb1 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m6,575,239\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m5,898,752\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_8 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │      \u001b[38;5;34m12,845,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_9 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1470\u001b[0m)                │         \u001b[38;5;34m754,110\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m30\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ efficientnetb1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,575,239</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,898,752</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1470</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">754,110</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,161,285\u001b[0m (126.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,161,285</span> (126.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,095,134\u001b[0m (126.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,095,134</span> (126.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m66,151\u001b[0m (258.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,151</span> (258.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=ROOT_DIR+\"model.weights.h5\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_freq=\"epoch\",\n",
        ")\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 40:\n",
        "    return 1e-3\n",
        "  elif epoch >= 40 and epoch < 80:\n",
        "    return 5e-4\n",
        "  else:\n",
        "    return 1e-4\n",
        "\n",
        "lr_callback = LearningRateScheduler(scheduler)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=LR),\n",
        "    loss=yolo_loss,\n",
        ")"
      ],
      "metadata": {
        "id": "RiyY-XGwDcHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(train_dataset, validation_data=val_dataset, verbose=1, callbacks=[checkpoint, lr_callback])"
      ],
      "metadata": {
        "id": "0G5VdBSjE5HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image=tf.io.decode_jpeg(tf.io.read_file(\"/content/drive/MyDrive/tfds_data/pascal_voc_2012/VOC2012/ValJPEGImages/2007_000515.jpg\"))\n",
        "image=tf.image.resize(image, [H,W])\n",
        "# model.predict(tf.expand_dims(image, axis = 0))"
      ],
      "metadata": {
        "id": "H-4tPRlKRHWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test(path, filename):\n",
        "  try:\n",
        "    test_path=path+filename\n",
        "    print(test_path)\n",
        "\n",
        "    img = cv.imread(test_path)\n",
        "    image=tf.io.decode_jpeg(tf.io.read_file(test_path))\n",
        "    image=tf.image.resize(image, [H,W])\n",
        "\n",
        "    output=model.predict(np.expand_dims(image, axis = 0))\n",
        "\n",
        "    THRESH=.60\n",
        "\n",
        "    \"\"\"\n",
        "    output[..., 0].shape            -> (1,7,7) (we take only the first score)\n",
        "    output[..., 5].shape            -> (1,7,7) (we take only the second score)\n",
        "    tf.where(output[...,0]>=THRESH) -> We get all positions (ex: [0, 3, 4]) where the first score is greater or equals to THRESH.\n",
        "    object_positions                -> All positions where the first or second score are greater or equal to THRESH.\n",
        "    selected_output                 -> Tensor with all size 30 arrays (from output, which has shape (1, 7, 7, 30)) whih contains 2 bboxes and the classes.\n",
        "    \"\"\"\n",
        "    object_positions=tf.concat(\n",
        "        [tf.where(output[...,0]>=THRESH), tf.where(output[...,5]>=THRESH)],\n",
        "        axis=0)\n",
        "    selected_output=tf.gather_nd(output, object_positions)\n",
        "    final_boxes=[]\n",
        "    final_scores=[]\n",
        "\n",
        "    # print(output.shape)\n",
        "    # print(output[...,0].shape)\n",
        "    # print(output[...,5].shape)\n",
        "    # print(tf.where(output[...,0]>=THRESH).shape)\n",
        "    # print(tf.where(output[...,5]>=THRESH).shape)\n",
        "    # print(object_positions.shape)\n",
        "    # print(selected_output.shape)\n",
        "\n",
        "    # return None # For printing\n",
        "\n",
        "    # EX: i=0, pos=[0,3,4]\n",
        "    for i, pos in enumerate(object_positions):\n",
        "      for j in range(2):\n",
        "        if selected_output[i][j*5]>THRESH:\n",
        "          \"\"\"\n",
        "          output[pos[0]][pos[1]][pos[2]][(j*5)+1:(j*5)+5]\n",
        "          >> if j == 0 -> we take from 1 to 5 (exclusive), which contains x_center, y_center, width and height of the first bbox.\n",
        "          >> if j == 1 -> we take from 6 to 10(exclusive), which contains x_center, y_center, width and height of the second bbox.\n",
        "          \"\"\"\n",
        "          output_box=tf.cast(output[pos[0]][pos[1]][pos[2]][(j*5)+1:(j*5)+5],dtype=tf.float32)\n",
        "\n",
        "          # >> pos[0] contains the batch (there's only one for this case).\n",
        "          # >> pos[1] contains the x cell of the image.\n",
        "          # >> pos[2] contains the y cell of the image.\n",
        "          # >> output_box[0] contains the x_center of the bbox.\n",
        "          # >> output_box[1] contains the y_center of the bbox.\n",
        "          # >> 32 (size of each cell): its the number used in ´SPLIT_SIZE = H//32´\n",
        "          # We sum x cell position with x_center and multiply it by 32 to get the exact x_center position of the bbox.\n",
        "          # We sum y cell position with y_center and multiply it by 32 to get the exact y_center position of the bbox.\n",
        "          x_center=(tf.cast(pos[1],dtype=tf.float32)+output_box[0])*32\n",
        "          y_center=(tf.cast(pos[2],dtype=tf.float32)+output_box[1])*32\n",
        "\n",
        "          # Unormalize width and height\n",
        "          x_width, y_height=tf.math.abs(W*output_box[2]),tf.math.abs(H*output_box[3])\n",
        "\n",
        "          x_min = int(x_center-(x_width/2))\n",
        "          y_min = int(y_center-(y_height/2))\n",
        "          x_max = int(x_center+(x_width/2))\n",
        "          y_max = int(y_center+(y_height/2))\n",
        "\n",
        "          x_min = 0 if x_min < 0 else x_min\n",
        "          y_min = 0 if y_min < 0 else y_min\n",
        "          x_max = W if x_max > W else x_max\n",
        "          y_max = H if y_max > H else y_max\n",
        "\n",
        "          # selected_output[...,10:] -> Because we only want the classes, so we skip the first two bboxes.\n",
        "          # [i] because selected_output and object_position both have the same number of rows. Ex:\n",
        "          # >> if   object_position.shape = (89,  3)\n",
        "          # >> then selected_output.shape = (89, 30)\n",
        "          final_boxes.append(\n",
        "              [x_min, y_min, x_max, y_max,\n",
        "              str(classes[tf.argmax(selected_output[...,10:],axis=-1)[i]])],\n",
        "          )\n",
        "          final_scores.append(selected_output[i][j*5])\n",
        "\n",
        "    print(\"finalscores\", final_scores)\n",
        "    print('finalboxes', final_boxes)\n",
        "    final_boxes=np.array(final_boxes)\n",
        "\n",
        "    object_classes=final_boxes[...,4]\n",
        "    nms_boxes=final_boxes[...,0:4]\n",
        "\n",
        "    \"\"\"\n",
        "    >> non_max_suppression: used to remove bboxes which predict the same object.\n",
        "    >> iou_threshold is used to tell if two bboxes are trying to predict the same object.\n",
        "    If the iou score is too low, it generally means the boxes are predicting different\n",
        "    objects.\n",
        "    >> score_threshold: discards all bboxes which have scores less than a defined amout\n",
        "    (even if they don't overlap using IOU score).\n",
        "    \"\"\"\n",
        "    nms_output=tf.image.non_max_suppression(\n",
        "        nms_boxes, final_scores, max_output_size=100,\n",
        "        iou_threshold=0.2, score_threshold=float('-inf'),\n",
        "    )\n",
        "    print(nms_output)\n",
        "\n",
        "    # nms_output has the index of the bboxes which passed\n",
        "    # through the non max suppresion operation.\n",
        "    for i in nms_output:\n",
        "      cv.rectangle(\n",
        "          img,\n",
        "          (int(final_boxes[i][0]), int(final_boxes[i][1])),\n",
        "          (int(final_boxes[i][2]), int(final_boxes[i][3])),(0,0,255),1)\n",
        "      cv.putText(\n",
        "          img,\n",
        "          final_boxes[i][-1],\n",
        "          (int(final_boxes[i][0]), int(final_boxes[i][1])+15),\n",
        "          cv.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "          1,\n",
        "          (0,225,0),\n",
        "          1,)\n",
        "\n",
        "    cv.imwrite('/content/'+ filename[:-4]+'_det'+'.jpg', cv.resize(img,(384,384)))\n",
        "  except:\n",
        "    print(\"NO object found !!!\")"
      ],
      "metadata": {
        "id": "B-svClov9n96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test(VAL_IMAGES, \"2007_000027.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCM5y0L84sJz",
        "outputId": "3b0bfc7f-cc53-4025-84a0-daca24b14af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/tfds_data/pascal_voc_2012/VOC2012/ValJPEGImages/2007_000027.jpg\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "finalscores [<tf.Tensor: shape=(), dtype=float32, numpy=0.75394267>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7138698>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6832388>, <tf.Tensor: shape=(), dtype=float32, numpy=0.71943706>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7093084>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6538927>, <tf.Tensor: shape=(), dtype=float32, numpy=0.74860483>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6531306>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7462428>, <tf.Tensor: shape=(), dtype=float32, numpy=0.67122984>, <tf.Tensor: shape=(), dtype=float32, numpy=0.62466246>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6106474>, <tf.Tensor: shape=(), dtype=float32, numpy=0.8205828>, <tf.Tensor: shape=(), dtype=float32, numpy=0.66371423>, <tf.Tensor: shape=(), dtype=float32, numpy=0.61115575>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7546419>, <tf.Tensor: shape=(), dtype=float32, numpy=0.62343615>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7067839>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6412621>, <tf.Tensor: shape=(), dtype=float32, numpy=0.76782745>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6058824>, <tf.Tensor: shape=(), dtype=float32, numpy=0.736102>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6567922>, <tf.Tensor: shape=(), dtype=float32, numpy=0.74860483>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6531306>, <tf.Tensor: shape=(), dtype=float32, numpy=0.779438>, <tf.Tensor: shape=(), dtype=float32, numpy=0.62466246>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6106474>, <tf.Tensor: shape=(), dtype=float32, numpy=0.8205828>, <tf.Tensor: shape=(), dtype=float32, numpy=0.66371423>, <tf.Tensor: shape=(), dtype=float32, numpy=0.781078>]\n",
            "finalboxes [[0, 49, 54, 224, 'bird'], [0, 38, 120, 173, 'cat'], [0, 166, 103, 224, 'boat'], [21, 0, 128, 71, 'bicycle'], [27, 15, 120, 153, 'bus'], [38, 0, 172, 61, 'bottle'], [81, 133, 145, 209, 'cat'], [88, 149, 134, 203, 'cat'], [88, 72, 223, 145, 'cat'], [126, 62, 178, 224, 'bicycle'], [125, 9, 224, 82, 'aeroplane'], [100, 14, 224, 92, 'aeroplane'], [134, 7, 213, 143, 'person'], [128, 40, 224, 123, 'person'], [135, 71, 193, 215, 'bird'], [151, 59, 224, 165, 'person'], [0, 0, 97, 56, 'tvmonitor'], [0, 13, 75, 76, 'train'], [0, 26, 107, 83, 'bird'], [14, 116, 97, 224, 'horse'], [30, 29, 117, 74, 'car'], [57, 53, 177, 121, 'diningtable'], [82, 86, 150, 194, 'cow'], [81, 133, 145, 209, 'cat'], [88, 149, 134, 203, 'cat'], [97, 0, 205, 91, 'cow'], [125, 9, 224, 82, 'aeroplane'], [100, 14, 224, 92, 'aeroplane'], [134, 7, 213, 143, 'person'], [128, 40, 224, 123, 'person'], [136, 154, 224, 224, 'sheep']]\n",
            "tf.Tensor([12 30 19  6 21  3], shape=(6,), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}